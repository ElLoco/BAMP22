{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Collection - Tweets per User**\n",
    "\n",
    "BAMP 2022 - MCT 4 - Ante Jelavic, Franziskus Perkhofer, Manuel Mencher, Melissa Ewering, Tim Ritzheimer\n",
    "\n",
    "Short description: This script is used to collect tweets using the Twitter API. The basis for this is the file \"Demographics.xlsx\" in which 100 Twitter user IDs and associated demographic data were collected manually. For these users, a 3 year history of all tweets will be retrieved (2019-2021).\n",
    "\n",
    "This script was written based on the Twitter API documentation, partly whole sections were copied and adjusted where necessary. Link: https://developer.twitter.com/en/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading of all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bearer Token is an unique token that is obtained when successfully applying for access to the Twitter API, as this is private data, it was removed after the cell was executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearer_token = 'To be filled'\n",
    "os.environ['TOKEN'] = bearer_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to create the specific search query / URL including all information about the specific request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url():\n",
    "    # Replace with user ID below\n",
    "    #user_id = 813286\n",
    "    return \"https://api.twitter.com/2/users/{}/tweets\".format(user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request details like the included attributes and the maximum amount of results are specified in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(pagination_token):\n",
    "    # Tweet fields are adjustable.\n",
    "    # Options include:\n",
    "    # attachments, author_id, context_annotations,\n",
    "    # conversation_id, created_at, entities, geo, id,\n",
    "    # in_reply_to_user_id, lang, non_public_metrics, organic_metrics,\n",
    "    # possibly_sensitive, promoted_metrics, public_metrics, referenced_tweets,\n",
    "    # source, text, and withheld\n",
    "    return {\n",
    "        'max_results': 100, \n",
    "        #\"tweet.fields\": \"id,text,author_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source\",\n",
    "        #\n",
    "        'expansions': 'author_id,in_reply_to_user_id,geo.place_id',\n",
    "        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',\n",
    "        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',\n",
    "        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',\n",
    "        #\n",
    "        'start_time':'2019-01-01T00:00:01Z', #START TIME selceted to get full year 2019 - 2021; 3 years\n",
    "        'end_time':'2022-01-01T00:00:01Z',\n",
    "        'pagination_token' : pagination_token\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two functions are used to establish the connection to the Twitter API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2UserTweetsPython\"\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(url, params):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence logic for the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general skeleton\n",
    "def retrieve_tweets ():\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    url = create_url()\n",
    "\n",
    "    end_criterion = 0\n",
    "    next_token = None # initialize for first run\n",
    "    while end_criterion == 0:\n",
    "      # perform web service call\n",
    "      params = get_params(next_token)\n",
    "      json_response = connect_to_endpoint(url, params)\n",
    "      df_tmp = pd.DataFrame(json_response['data'])\n",
    "      try:\n",
    "        df = df.append(df_tmp)\n",
    "        print(\"Log: df_tmp appended to df\")\n",
    "      except NameError:\n",
    "        # should only appear the first time\n",
    "        print(\"Log: First run, set df with df_tmp\")\n",
    "        df = df_tmp\n",
    "      print(\"Log: df has now a length of \" + str(len(df)))\n",
    "\n",
    "      try:\n",
    "        next_token = json_response['meta']['next_token']\n",
    "      except KeyError:\n",
    "        print(\"Log: Caught Error - No next token available.\")\n",
    "        next_token = None\n",
    "\n",
    "      if not next_token:\n",
    "        print(\"Log: No next token available, all tweets retrieved\")\n",
    "        end_criterion = 1\n",
    "    #  if len(df) >= max_tweets:\n",
    "    #    print(\"Log: Maximal number of \" + str(max_tweets) + \" tweets we want to collect reached.\")\n",
    "    #    end_criterion = 1\n",
    "      if end_criterion == 0:\n",
    "        # only wait if the loop continues\n",
    "        time.sleep(3) # Sleep for 3 seconds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the sequence logic in a for loop to retrieve the tweets for all 100 user ids (Exemplary output for only 2 user ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 99\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 199\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 286\n",
      "Log: Caught Error - No next token available.\n",
      "Log: No next token available, all tweets retrieved\n",
      "Completed: 1\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 100\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 200\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 300\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 400\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 500\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 600\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 700\n",
      "200\n",
      "Log: df_tmp appended to df\n",
      "Log: df has now a length of 715\n",
      "Log: Caught Error - No next token available.\n",
      "Log: No next token available, all tweets retrieved\n",
      "Completed: 2\n"
     ]
    }
   ],
   "source": [
    "df_output = pd.DataFrame()\n",
    "\n",
    "df_input = pd.read_excel('Demographics.xlsx') #Needs to be saved in \"Scripts\" folder for runtime\n",
    "\n",
    "counter = 1\n",
    "for x in df_input[\"ID\"]:\n",
    "    user_id = x\n",
    "    df_output = df_output.append(retrieve_tweets())\n",
    "    print(\"Completed: \" + str(counter))\n",
    "    counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the output to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df_output.replace('\\n', '', regex = True)\n",
    "df_output.to_csv(\"Tweets.csv\") # File is saved in the \"Scripts\" folder and will be afterwards moved to output folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
